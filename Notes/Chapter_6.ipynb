{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Continuation of Fixed Points\n",
    "\n",
    "We have learned to find fixed points earlier. This requires solving nonlinear algbraic equations of the form\n",
    "\n",
    "$$\\mathbf{0} = \\mathbf{F}(\\mathbf{x},\\mathbf{M})$$ \n",
    "for a given value of control parameter/s $\\mathbf{M}$. Let's define a solution to this to be $\\mathbf{x}_0$ for $\\mathbf{M}_0$. \n",
    "\n",
    "If the Jacobian matrix (we also called it $A$) $D_\\mathbf{x}\\mathbf{F}(\\mathbf{x},\\mathbf{M})$ is not singular, then for each $\\mathbf{M}$ in the vicinity of $\\mathbf{x}_0, \\mathbf{M}_0$ there is a unique solution for $\\mathbf{x}$. (*Implicite function theorem, Hale, 1969*)\n",
    "\n",
    "This solution is given by \n",
    "\n",
    "$$\\mathbf{x} = \\mathbf{G}(\\mathbf{M})$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\mathbf{x}_0 = \\mathbf{G}(\\mathbf{M}_0)$$\n",
    "\n",
    "#### Example 6.1\n",
    "\n",
    "$$\\dot{x} = \\mu-x^2$$\n",
    "\n",
    "At the bifurcation point $(0,0)$: \n",
    "$$F_x = 0,\\;\\text{and}\\; F_{\\mu}=1$$\n",
    "\n",
    "\n",
    "Because $F_x=0$, the dependence of $x$ on $\\mu$ is *not* unique. \n",
    "\n",
    "However, because $F_{\\mu}\\neq 0$ there is a unique function $\\mu(x)$ in the vicinity of the bifurcation point \n",
    "\n",
    "### 6.1.1 Sequential Continuation\n",
    "\n",
    "Here the control parameter is called $\\alpha$ (I have no idea why the name changes again).\n",
    "\n",
    "$\\alpha$ is divided into closely spaced intervals $\\alpha_0, \\alpha_1, \\alpha_2, \\ldots \\alpha_n$\n",
    "\n",
    "- The solution for $\\mathbf{x}_j$ at $\\alpha_j$ is used as the initial predicted guess for the solution $\\mathbf{x}_{j+1}(0)$  at $\\alpha_{j+1}$. \n",
    "\n",
    "- We iterate on $\\mathbf{x}_{j+1}^{(k)}$ using\n",
    "\n",
    "$$\\mathbf{x}_{j+1}^{(k+1)}=\\mathbf{x}_{j+1}^{(k)}+r\\Delta \\mathbf{x}^{(k)}$$\n",
    "\n",
    "where $k$ is the iteration number. \n",
    "\n",
    "$r$ is such that $0<r\\le 1$ is the *relaxation parameter* and $\\Delta\\mathbf{x}^{(k)}$ is the solution of \n",
    "\n",
    "$$\\mathbf{F}_{\\mathbf{x}}\\left(\\mathbf{x}_{j+1}^{(k)},\\alpha_{j+1}   \\right)\\Delta\\mathbf{x}^{(k)} = -\\mathbf{F}\\left(\\mathbf{x}_{j+1}^{(k)},\\alpha_{j+1}   \\right)$$\n",
    "\n",
    "which can be solved by Gauss elimination provided that the matrix  $\\mathbf{F}_{\\mathbf{x}}\\left(\\mathbf{x}_{j+1}^{(k)},\\alpha_{j+1}   \\right)$ is non-singular (recall that condition)?\n",
    "\n",
    "The relaxation parameter is obtained vie Newton-Raphson iteration  (or Golden Section, etc.) such that \n",
    "\n",
    "$$\\left| \\mathbf{F}\\!\\left(\\mathbf{x}_{j+1}^{(k+1)},\\alpha_{j+1}   \\right)  \\right|\n",
    "<\\left| \\mathbf{F}\\!\\left(\\mathbf{x}_{j+1}^{(k)},\\alpha_{j+1}   \\right)  \\right|$$\n",
    "\n",
    "This is actually a simple gradient search algorithm. Other minimization techniques could be used and would likely be much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davidenko-Newton-Raphson Continuation\n",
    "\n",
    "Differentiating \n",
    "\n",
    "$$\\mathbf{0} = \\mathbf{F}(\\mathbf{x},\\alpha)$$ \n",
    "with respect to $\\alpha$ yields\n",
    "\n",
    "$$\\mathbf{F}_{\\mathbf{x}}(\\mathbf{x},\\alpha)\\frac{d\\mathbf{x}}{d\\alpha} = -\\mathbf{F}_{\\alpha}(\\mathbf{x},\\alpha)$$ \n",
    "\n",
    "The is a set of $n$ linear equations in the unknown $\\frac{d\\mathbf{x}}{d\\alpha}$\n",
    "\n",
    "Hence, we can premultiply by the inverse of $\\mathbf{F}_{\\mathbf{x}}(\\mathbf{x},\\alpha)$ to obtain\n",
    "\n",
    "$$\\frac{d\\mathbf{x}}{d\\alpha}=-\\mathbf{F}_{\\mathbf{x}}^{-1}(\\mathbf{x},\\alpha)\\mathbf{F}_{\\alpha}(\\mathbf{x},\\alpha)$$\n",
    "\n",
    "which then given \n",
    "$$\\mathbf{x}(\\alpha_0)= \\mathbf{x}_0$$\n",
    "\n",
    "we can numerically integrate this to obtain the dependence of $\\mathbf{x}$ on $\\alpha$ (for instance Runge-Kutta) \n",
    "\n",
    "Note that this method, like the previous method, will fail at turning points and branch points due to singularity of $\\mathbf{F}_{\\mathbf{x}}^{-1}(\\mathbf{x},\\alpha)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arclength Continuation\n",
    "\n",
    "In this method, $\\mathbf{x}$ and $\\alpha$ are considered to be a function of $s$, a position along an arc illustrated below. \n",
    "\n",
    "<img src=\"Figure6.1.2.png\">\n",
    "\n",
    "We are to trace the branch we are looking to satisfy\n",
    "\n",
    "$$\\mathbf{F}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)=\\mathbf{0}$$\n",
    "\n",
    "Differentiating this with respect to $s$ yields\n",
    "\n",
    "$$\\mathbf{F}_{\\mathbf{x}}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\\frac{d\\mathbf{x}}{ds}+\\mathbf{F}_{\\alpha}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\\frac{d\\alpha}{ds}=\\mathbf{0}$$\n",
    "\n",
    "which can be written in matrix form as\n",
    "\n",
    "\\begin{split}\n",
    "&\\begin{bmatrix}\\mathbf{F}_{\\mathbf{x}}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)&\\mathbf{F}_{\\alpha}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{d\\mathbf{x}}{ds}\\\\\n",
    "\\frac{d\\alpha}{ds}\n",
    "\\end{bmatrix}\\\\\n",
    "&= \n",
    "\\begin{bmatrix}\\mathbf{F}_{\\mathbf{x}}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)&\\mathbf{F}_{\\alpha}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\n",
    "\\end{bmatrix}\n",
    "\\mathbf{t}\n",
    "=\n",
    "\\mathbf{0}\n",
    "\\end{split}\n",
    "\n",
    "where $\\mathbf{t}$ is a vector tangent to the curve. \n",
    "\n",
    "This is a set of $n$ linear equations in the now $n+1$ unknowns, so we still can't solve them. \n",
    "\n",
    "To specify them uniquely, the solution $\\mathbf{t}$ is typically required to be a unit vector. \n",
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\frac{d\\mathbf{x}}{ds}^T&\n",
    "\\frac{d\\alpha}{ds}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\frac{d\\mathbf{x}}{ds}\\\\\n",
    "\\frac{d\\alpha}{ds}\n",
    "\\end{bmatrix} = 0\n",
    "$$\n",
    "\n",
    "In practice, this will make the equations harder to solve as you now have a nonlinear set of equations. One could more easily set $\\frac{d\\alpha}{ds}=1$ then solve for $\\frac{d\\mathbf{x}}{ds}$ and scale the result to a length of $1$. \n",
    "\n",
    "Once $\\frac{d\\mathbf{x}}{ds}$ and $\\frac{d\\alpha}{ds}=1$ are known, the initial conditions\n",
    "\n",
    "$$\\mathbf{x} = \\mathbf{x}_0\\;\\text{and}\\;\\alpha = \\alpha_0\\;\\text{at}\\;s=0$$\n",
    "\n",
    "The text states that, with a lot of math... you should do like I suggested, but write it in a convoluted way. \n",
    "\n",
    "We then perform numerical integration forward in $s$ such that\n",
    "\n",
    "$$\\mathbf{x} = \\mathbf{x}_0+\\frac{d\\mathbf{x}}{ds}\\Delta s$$\n",
    "and\n",
    "$$\\alpha = \\alpha_0+\\frac{d\\alpha}{ds}\\Delta s$$\n",
    "\n",
    "This now is simply a matter of performing accurate numerical integrations, which can use any numerical integration algorithm, many of which Matlab, Python, etc. have as tools. Simply replace *time* in those methods with $s$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Simple Turning and Branch Points\n",
    "\n",
    "We can determine turning and branch points through **direct** or **indirect** methods. \n",
    "\n",
    "- *Indirect methods* yield them as a by-product of continuation methods. \n",
    "- The rank of the Jacobian matrix is monitored for a drop. \n",
    "- When one of the eigenvalues is zero (near zero), the rank is $n-1$ and there is either a turning point or a branch point. \n",
    "- The rank of $\\left[ \\mathbf{F}_{\\mathbf{x}}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\\;\\mathbf{F}_{\\alpha}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\\right]$ is used to determine whether it's a turning point or branch point. \n",
    "\n",
    "- *Direct methods* are more expensive, but more accurate. \n",
    "\n",
    "Noting that \n",
    "$$\\mathbf{F}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)=\\mathbf{0}$$\n",
    "$$\\mathbf{F}_{\\mathbf{x}}\\!\\left(\\mathbf{x}(s),\\alpha(s)  \\right)\\mathbf{u}=\\mathbf{0}$$\n",
    "(which means there is a zero eigenvalue)\n",
    "\n",
    "and\n",
    "$$\\mathbf{u}^T\\mathbf{u}=1$$\n",
    "or another normalization method. \n",
    "\n",
    "These 3 equations can be solved using a Newton-Raphson method.\n",
    "\n",
    "$$\\mathbf{F}_{\\mathbf{x}}^k\\Delta \\mathbf{x}+\\mathbf{F}_{\\alpha}^k\\Delta \\alpha = -\\mathbf{F}^k$$\n",
    "\n",
    "$$\\left(\\mathbf{F}_{\\mathbf{x}\\mathbf{x}}^k \\mathbf{u}^k\\right)\\Delta \\mathbf{x}+\\left(\\mathbf{F}_{\\mathbf{x}\\alpha}^k\\mathbf{u}^k\\right)\\Delta \\alpha + \\mathbf{F}_{\\mathbf{x}}^k\\Delta \\mathbf{u}= -\\mathbf{F}_\\mathbf{x}^k\\mathbf{u}^k$$\n",
    "and\n",
    "$$2\\mathbf{u}^T\\Delta\\mathbf{u}=1-\\left(\\mathbf{u}^T\\mathbf{u}\\right)^k$$\n",
    "\n",
    "$\\mathbf{F}_{\\mathbf{x}\\mathbf{x}}$ represents the matrix of second partial derivatives with respect to $\\mathbf{x}$ and $\\mathbf{F}_{\\mathbf{x}\\alpha}$ represents the matrix of second partial derivatives with respect to $\\mathbf{x}$ and $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hopf Bifurcations\n",
    "\n",
    "At a Hopf Bifurcation we note that the Jacobian matrix has a paris of purely imaginary values, with all the other eigenvalues having nonzero real parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
